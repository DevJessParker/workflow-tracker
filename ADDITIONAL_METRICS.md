# Additional Metrics & Analytics for Repository Analysis
## Comprehensive Feature Recommendations for Development Teams

**Research Date**: November 3, 2025
**Purpose**: Identify valuable metrics beyond current implementation

---

## Executive Summary

Based on industry research and competitive analysis, development teams need **6 major categories** of metrics. Your current implementation covers ~15% of these. Adding the recommendations below would create a truly comprehensive platform.

**Priority Rating System**:
- üî¥ **Critical** - Must-have for market competitiveness
- üü° **High Value** - Strong differentiator, frequently requested
- üü¢ **Nice to Have** - Valuable but not urgent
- üîµ **Future** - Interesting but low priority

---

## Category 1: Code Quality & Technical Debt

### 1.1 Code Complexity Metrics üî¥ CRITICAL

**What**: Measure code complexity to identify hard-to-maintain areas

**Metrics**:
- **Cyclomatic Complexity**: Number of independent paths through code
  - Target: <10 per function
  - Warning: 11-20
  - Critical: >20
- **Cognitive Complexity**: How hard code is to understand (newer, better metric)
- **Lines of Code (LOC)**: Total lines, by file/module
- **Function Length**: Lines per function (target: <50 lines)
- **File Length**: Lines per file (target: <500 lines)
- **Nesting Depth**: How many levels of indentation (target: <4)

**Implementation**:
```python
@dataclass
class ComplexityMetrics:
    file_path: str
    function_name: str
    cyclomatic_complexity: int  # McCabe complexity
    cognitive_complexity: int  # SonarSource metric
    lines_of_code: int
    nesting_depth: int
    complexity_rating: str  # low, medium, high, critical
    location: CodeLocation
```

**Visualization**:
- Heat map: Files colored by complexity (green ‚Üí red)
- List: Top 10 most complex functions
- Trend: Complexity over time (increasing = bad)
- Distribution: How many functions at each complexity level

**Value**: Helps teams prioritize refactoring efforts

**Estimated Effort**: 2-3 weeks

---

### 1.2 Code Duplication Detection üî¥ CRITICAL

**What**: Find copy-pasted code that should be refactored

**Metrics**:
- **Duplication Percentage**: % of codebase that's duplicated
  - Target: <5%
  - Warning: 5-10%
  - Critical: >10%
- **Clone Blocks**: Specific instances of duplicated code
- **Clone Families**: Groups of similar duplicates

**Implementation**:
```python
@dataclass
class CodeClone:
    clone_id: str
    locations: List[CodeLocation]  # Where duplicates exist
    lines_duplicated: int
    similarity_percent: float  # 100% = exact copy
    code_snippet: str
    refactoring_suggestion: str  # Extract to function, create utility

@dataclass
class DuplicationReport:
    total_duplication_percent: float
    clone_blocks: List[CodeClone]
    files_with_most_duplication: List[str]
    estimated_lines_to_save: int  # If refactored
```

**Visualization**:
- List of duplicate blocks with side-by-side comparison
- Files with most duplication
- Trend over time

**Value**: Reduces maintenance burden, prevents bugs

**Estimated Effort**: 2-3 weeks

---

### 1.3 Technical Debt Tracking üü° HIGH VALUE

**What**: Quantify and track technical debt over time

**Metrics**:
- **Debt Ratio**: (Remediation cost / Development cost) √ó 100
  - Target: <5%
  - Warning: 5-10%
  - Critical: >10%
- **Remediation Time**: Estimated hours to fix all issues
- **Debt Trend**: Is debt increasing or decreasing?
- **Debt by Category**:
  - Code smells (45%)
  - Test debt (25%)
  - Documentation debt (15%)
  - Architecture debt (15%)

**Implementation**:
```python
@dataclass
class TechnicalDebtItem:
    category: str  # code_smell, test_debt, doc_debt, architecture
    severity: str  # minor, major, critical
    location: CodeLocation
    description: str
    remediation_time_hours: float
    introduced_date: datetime
    assigned_to: Optional[str]

@dataclass
class TechnicalDebtReport:
    total_debt_hours: float
    debt_ratio: float
    debt_by_category: Dict[str, float]
    trending: str  # increasing, stable, decreasing
    top_debt_items: List[TechnicalDebtItem]
```

**Visualization**:
- Debt ratio gauge (green/yellow/red)
- Trend chart over time
- Breakdown by category
- List of actionable debt items

**Value**: Makes invisible technical debt visible to stakeholders

**Estimated Effort**: 3-4 weeks

---

### 1.4 Code Smells Detection üü° HIGH VALUE

**What**: Identify common anti-patterns and bad practices

**Common Code Smells**:
- **Long Method** (>50 lines)
- **Long Parameter List** (>4 parameters)
- **Large Class** (>500 lines or >20 methods)
- **God Object** (class that does too much)
- **Duplicate Code** (see 1.2)
- **Dead Code** (unused functions, variables)
- **Magic Numbers** (hardcoded values)
- **Deeply Nested Conditionals** (>3 levels)
- **Switch Statement Abuse** (should use polymorphism)
- **Feature Envy** (method uses another class more than its own)
- **Primitive Obsession** (using primitives instead of objects)

**Implementation**:
```python
class CodeSmellType(Enum):
    LONG_METHOD = "long_method"
    LONG_PARAMETER_LIST = "long_parameter_list"
    LARGE_CLASS = "large_class"
    GOD_OBJECT = "god_object"
    DEAD_CODE = "dead_code"
    MAGIC_NUMBER = "magic_number"
    DEEP_NESTING = "deep_nesting"
    FEATURE_ENVY = "feature_envy"

@dataclass
class CodeSmell:
    smell_type: CodeSmellType
    location: CodeLocation
    severity: str  # info, minor, major
    description: str
    suggestion: str  # How to fix
    affected_code: str
```

**Value**: Improves code maintainability and readability

**Estimated Effort**: 3-4 weeks

---

## Category 2: Security & Vulnerability Analysis

### 2.1 Vulnerability Scanning üî¥ CRITICAL

**What**: Detect security vulnerabilities in dependencies and code

**Metrics**:
- **Critical Vulnerabilities**: Immediate action required
- **High Vulnerabilities**: Should fix soon
- **Medium/Low Vulnerabilities**: Nice to fix
- **CVSS Score**: Common Vulnerability Scoring System
- **Exploitability**: Is there a known exploit?

**Data Sources**:
- **npm audit** (JavaScript/TypeScript)
- **pip-audit** (Python)
- **OWASP Dependency-Check** (Java, .NET)
- **GitHub Advisory Database**
- **Snyk Vulnerability Database**

**Implementation**:
```python
@dataclass
class Vulnerability:
    cve_id: str  # e.g., CVE-2024-12345
    package_name: str
    affected_versions: str
    fixed_version: Optional[str]
    severity: str  # critical, high, medium, low
    cvss_score: float  # 0-10
    exploitability: str  # unproven, proof_of_concept, functional, high
    description: str
    remediation: str
    references: List[str]  # Links to advisories

@dataclass
class SecurityReport:
    critical_count: int
    high_count: int
    medium_count: int
    low_count: int
    vulnerabilities: List[Vulnerability]
    risk_score: float  # Overall security risk (0-100)
```

**Visualization**:
- Security risk gauge
- List of vulnerabilities by severity
- Dependency tree showing vulnerable packages
- Timeline: When vulnerabilities were introduced

**Value**: Prevents security breaches, compliance requirement

**Estimated Effort**: 3-4 weeks

---

### 2.2 Secret Detection üî¥ CRITICAL

**What**: Find accidentally committed secrets (API keys, passwords, tokens)

**Patterns to Detect**:
- API keys (AWS, Azure, Google Cloud, Stripe, etc.)
- Database connection strings
- Private keys (RSA, SSH)
- Passwords in code
- OAuth tokens
- JWT secrets
- Hardcoded credentials

**Implementation**:
```python
@dataclass
class SecretLeak:
    secret_type: str  # aws_key, database_password, private_key, etc.
    location: CodeLocation
    severity: str  # critical, high
    exposed_value: str  # Partially masked: "sk_live_***********"
    recommendation: str  # "Rotate immediately, use environment variables"
    first_seen_commit: str
    still_present: bool

@dataclass
class SecretScanReport:
    total_secrets_found: int
    critical_secrets: List[SecretLeak]
    historical_secrets: List[SecretLeak]  # In git history but removed
    remediation_steps: List[str]
```

**Value**: Prevents data breaches, critical security feature

**Estimated Effort**: 2-3 weeks

---

### 2.3 OWASP Top 10 Detection üü° HIGH VALUE

**What**: Detect common security issues from OWASP Top 10

**Patterns**:
1. **SQL Injection**: Unsanitized user input in SQL queries
2. **XSS (Cross-Site Scripting)**: Unsanitized HTML output
3. **Insecure Authentication**: Weak password rules, missing MFA
4. **Sensitive Data Exposure**: Unencrypted sensitive data
5. **XML External Entities (XXE)**
6. **Broken Access Control**: Missing authorization checks
7. **Security Misconfiguration**: Debug mode in production
8. **Insecure Deserialization**
9. **Using Components with Known Vulnerabilities** (see 2.1)
10. **Insufficient Logging & Monitoring**

**Value**: Improves application security posture

**Estimated Effort**: 4-5 weeks (complex)

---

## Category 3: Performance & Scalability

### 3.1 Performance Anti-Patterns üü° HIGH VALUE

**What**: Detect code patterns that hurt performance

**Patterns**:
- **N+1 Database Queries**: Queries in loops
  ```csharp
  // ‚ùå BAD - N+1 query
  foreach (var user in users) {
      var posts = _db.Posts.Where(p => p.UserId == user.Id).ToList();
  }

  // ‚úÖ GOOD - Single query with join
  var usersWithPosts = _db.Users.Include(u => u.Posts).ToList();
  ```

- **Missing Database Indexes**: Queries on un-indexed columns
- **Inefficient Algorithms**: O(n¬≤) when O(n log n) exists
- **Memory Leaks**: Objects not disposed, event handlers not unsubscribed
- **Large Object Graphs**: Loading too much data at once
- **Synchronous I/O in Async Context**: Blocking calls in async methods

**Implementation**:
```python
@dataclass
class PerformanceIssue:
    issue_type: str  # n_plus_one, missing_index, inefficient_algorithm
    location: CodeLocation
    severity: str  # critical, high, medium
    description: str
    impact: str  # "Could cause 10x slowdown under load"
    suggestion: str  # How to fix
    code_before: str
    code_after: str  # Suggested improvement

@dataclass
class PerformanceReport:
    total_issues: int
    critical_issues: List[PerformanceIssue]
    estimated_impact: str  # "Could improve response time by 50%"
```

**Value**: Prevents production performance problems

**Estimated Effort**: 4-5 weeks

---

### 3.2 Resource Usage Analysis üü¢ NICE TO HAVE

**What**: Estimate memory and CPU usage

**Metrics**:
- Estimated memory allocations
- Large object creation
- CPU-intensive operations (nested loops, heavy computation)
- File handle usage
- Network connections

**Value**: Helps optimize resource usage

**Estimated Effort**: 3-4 weeks

---

## Category 4: Testing & Quality Assurance

### 4.1 Test Quality Metrics üî¥ CRITICAL

**What**: Beyond coverage percentage, measure test effectiveness

**Metrics**:
- **Test Coverage**: % of lines/branches/functions covered
- **Mutation Score**: % of mutants killed (mutation testing)
- **Test-to-Code Ratio**: Lines of test code vs production code (target: 1:1)
- **Assertion Density**: Assertions per test (target: 3-5)
- **Test Flakiness**: Tests that randomly fail
- **Test Execution Time**: Slow tests (>1s per test)
- **Test Complexity**: Complex tests are hard to maintain

**Implementation**:
```python
@dataclass
class TestQualityMetrics:
    test_file: str
    test_count: int
    assertion_count: int
    assertion_density: float  # assertions / tests
    avg_test_length: int  # lines per test
    slow_tests: List[str]  # Tests taking >1s
    flaky_tests: List[str]  # Failed in last 10 runs
    tests_without_assertions: List[str]
    mutation_score: float  # % of mutants killed (if available)

@dataclass
class TestCoverageReport:
    line_coverage_percent: float
    branch_coverage_percent: float
    function_coverage_percent: float
    uncovered_critical_code: List[CodeLocation]  # High-risk uncovered
    test_quality: TestQualityMetrics
```

**Visualization**:
- Coverage heat map (file-level)
- Uncovered critical paths (database writes, API calls)
- Flaky test list with history
- Slow test list

**Value**: Better tests = higher code quality

**Estimated Effort**: 3-4 weeks

---

### 4.2 Test Recommendations üü° HIGH VALUE

**What**: AI-suggested areas that need more tests

**Recommendations**:
- "Function `UserService.DeleteUser` has no tests"
- "Database write operation in `OrderController.CreateOrder` is untested"
- "Error path in `PaymentService.ProcessPayment` has no tests"
- "Complex function `CalculateShipping` (complexity 15) is untested"

**Implementation**:
```python
@dataclass
class TestRecommendation:
    priority: str  # critical, high, medium
    location: CodeLocation
    reason: str  # "Untested database write operation"
    risk_level: str  # "High risk if this fails in production"
    suggested_test: str  # Example test code
```

**Value**: Guides developers to write the right tests

**Estimated Effort**: 3-4 weeks

---

## Category 5: Documentation & Knowledge

### 5.1 Documentation Coverage üü° HIGH VALUE

**What**: Measure quality of code documentation

**Metrics**:
- **Comment Density**: Comments per 100 lines of code (target: 15-25%)
- **Public API Documentation**: % of public functions with docs
- **Complex Code Documentation**: % of complex functions with comments
- **Outdated Comments**: Comments contradicting code
- **README Quality**: Does README exist and is it comprehensive?
- **API Documentation**: Swagger/OpenAPI for all endpoints

**Implementation**:
```python
@dataclass
class DocumentationMetrics:
    total_functions: int
    documented_functions: int
    documentation_coverage: float  # %
    undocumented_public_apis: List[str]
    undocumented_complex_functions: List[str]  # complexity >10
    comment_density: float  # % of lines that are comments
    readme_exists: bool
    readme_sections: List[str]  # Sections found in README
    api_docs_coverage: float  # % of endpoints documented

@dataclass
class DocumentationGap:
    location: CodeLocation
    function_name: str
    reason: str  # "Public API missing documentation"
    priority: str  # high, medium, low
```

**Value**: Improves onboarding, reduces confusion

**Estimated Effort**: 2-3 weeks

---

### 5.2 Knowledge Distribution üü° HIGH VALUE

**What**: Identify knowledge silos and "bus factor"

**Metrics**:
- **Bus Factor**: How many developers need to leave before knowledge is lost? (target: >2)
- **Code Ownership**: Who touches which files most?
- **Knowledge Silos**: Files only one person has edited
- **Expertise Distribution**: Is knowledge spread or concentrated?

**Implementation**:
```python
@dataclass
class KnowledgeMetrics:
    file_path: str
    primary_author: str  # Person with most commits
    primary_author_percentage: float  # % of commits
    contributor_count: int
    bus_factor: int  # How many people understand this file?
    last_edited: datetime
    risk_level: str  # high (bus factor 1), medium (2), low (3+)

@dataclass
class TeamKnowledgeReport:
    overall_bus_factor: int
    knowledge_silos: List[KnowledgeMetrics]  # Single-person files
    most_concentrated_knowledge: List[KnowledgeMetrics]
    recommendations: List[str]  # "Pair programming on FileX recommended"
```

**Value**: Reduces risk, improves team resilience

**Estimated Effort**: 2-3 weeks (uses git data)

---

## Category 6: Development Velocity & Productivity

### 6.1 DORA Metrics üî¥ CRITICAL

**What**: The four key metrics for DevOps performance

**The Four DORA Metrics**:

1. **Lead Time for Changes**
   - Time from commit to production
   - Elite: <1 hour
   - High: 1 day - 1 week
   - Medium: 1 week - 1 month
   - Low: 1-6 months

2. **Deployment Frequency**
   - How often code is deployed
   - Elite: Multiple times per day
   - High: Once per day to once per week
   - Medium: Once per week to once per month
   - Low: Less than once per month

3. **Mean Time to Restore (MTTR)**
   - How long to recover from failure
   - Elite: <1 hour
   - High: <1 day
   - Medium: 1 day - 1 week
   - Low: 1 week - 1 month

4. **Change Failure Rate**
   - % of deployments causing production failure
   - Elite: 0-15%
   - High: 16-30%
   - Medium: 31-45%
   - Low: 46-100%

**Implementation**:
```python
@dataclass
class DORAMetrics:
    lead_time_hours: float
    deployment_frequency_per_day: float
    mttr_hours: float
    change_failure_rate: float  # 0-1
    performance_tier: str  # elite, high, medium, low
    trend: str  # improving, stable, declining
```

**Value**: Industry standard for DevOps maturity

**Estimated Effort**: 3-4 weeks

---

### 6.2 Pull Request Metrics üü° HIGH VALUE

**What**: Measure PR health and efficiency

**Metrics**:
- **PR Cycle Time**: Time from open to merge
  - Target: <24 hours
  - Warning: >48 hours
- **PR Size**: Lines changed
  - Target: <200 lines
  - Warning: >500 lines (hard to review)
- **Time to First Review**: How long before first comment
- **Review Iterations**: How many back-and-forths
- **PR Merge Rate**: % of PRs that get merged (vs closed)
- **Stale PRs**: PRs open >7 days without activity

**Implementation**:
```python
@dataclass
class PullRequestMetrics:
    pr_id: str
    author: str
    opened_at: datetime
    merged_at: Optional[datetime]
    cycle_time_hours: float
    lines_changed: int
    time_to_first_review_hours: float
    review_iterations: int
    reviewers: List[str]
    status: str  # open, merged, closed
    comments: int

@dataclass
class PRHealthReport:
    avg_cycle_time_hours: float
    avg_pr_size_lines: int
    avg_time_to_first_review: float
    stale_prs: List[PullRequestMetrics]
    large_prs: List[PullRequestMetrics]  # >500 lines
    team_performance: str  # fast, moderate, slow
```

**Value**: Improves code review process

**Estimated Effort**: 2-3 weeks

---

### 6.3 Flow Metrics üü° HIGH VALUE

**What**: Measure work-in-progress and bottlenecks

**Metrics**:
- **Work in Progress (WIP)**: Active branches/PRs per developer
  - Target: 1-2
  - Warning: >3
- **Cycle Time**: Start to finish for a feature
- **Flow Efficiency**: Active time / total time (target: >40%)
- **Blocked Time**: How long PRs wait for review/approval
- **Batch Size**: How many changes per deployment

**Value**: Identifies process bottlenecks

**Estimated Effort**: 2-3 weeks

---

### 6.4 Code Churn Analysis üü¢ NICE TO HAVE

**What**: How often code is changed after being written

**Metrics**:
- **Code Churn Rate**: Lines changed / total lines (per file, per time period)
- **High Churn Files**: Files changed frequently (may indicate instability)
- **Refactoring vs New Code**: % of commits that are refactoring
- **Rework**: Code rewritten within 2 weeks of being written

**Implementation**:
```python
@dataclass
class ChurnMetrics:
    file_path: str
    churn_rate: float  # 0-1, higher = more volatile
    times_modified: int  # Last 30 days
    authors_touching: int  # How many different people
    is_hotspot: bool  # High complexity + high churn = risky
    stability_score: float  # 0-1, higher = more stable
```

**Value**: Identifies unstable areas of codebase

**Estimated Effort**: 2-3 weeks

---

## Category 7: Architecture & Design

### 7.1 Modularity Metrics üü° HIGH VALUE

**What**: Measure how well code is organized

**Metrics**:
- **Coupling**: How connected modules are (low is better)
  - Afferent Coupling (Ca): How many modules depend on this
  - Efferent Coupling (Ce): How many modules this depends on
- **Cohesion**: How focused modules are (high is better)
- **Instability**: I = Ce / (Ca + Ce)
  - 0 = stable, 1 = unstable
- **Abstractness**: % of abstract classes/interfaces

**Implementation**:
```python
@dataclass
class ModuleMetrics:
    module_name: str
    afferent_coupling: int  # Incoming dependencies
    efferent_coupling: int  # Outgoing dependencies
    instability: float  # 0-1
    abstractness: float  # 0-1
    distance_from_main_sequence: float  # Quality metric
    classes: int
    interfaces: int
```

**Value**: Guides architectural improvements

**Estimated Effort**: 3-4 weeks

---

### 7.2 Circular Dependency Detection üü° HIGH VALUE

**What**: Find modules that depend on each other (bad practice)

**Example**:
```
UserService ‚Üí OrderService ‚Üí PaymentService ‚Üí UserService
                                  ‚Üë________________|
                                  (circular!)
```

**Value**: Prevents tight coupling, improves testability

**Estimated Effort**: 2-3 weeks

---

### 7.3 Layer Violation Detection üü¢ NICE TO HAVE

**What**: Ensure architectural layers are respected

**Example Violation**:
```
UI Layer ‚Üí Data Layer (skips Business Layer) ‚ùå
```

**Value**: Maintains clean architecture

**Estimated Effort**: 3-4 weeks

---

## Category 8: Collaboration & Team Health

### 8.1 Code Review Quality üü° HIGH VALUE

**What**: Measure effectiveness of code reviews

**Metrics**:
- **Review Coverage**: % of commits that went through PR review
- **Superficial Reviews**: Approved in <5 minutes with no comments
- **Review Depth**: Average comments per PR
- **Review Response Time**: Time to address review comments
- **Reviewer Distribution**: Are a few people reviewing everything?

**Value**: Improves code review culture

**Estimated Effort**: 2-3 weeks

---

### 8.2 Pair Programming Metrics üü¢ NICE TO HAVE

**What**: Track collaboration patterns

**Metrics**:
- **Co-Authorship**: How often developers commit together
- **Collaboration Network**: Who works with whom
- **Mentorship Indicators**: Senior/junior collaboration

**Value**: Encourages collaboration, knowledge sharing

**Estimated Effort**: 2-3 weeks

---

## Priority Matrix: What to Build Next

### Critical (Build First) üî¥
1. **Code Complexity Metrics** (2-3 weeks) - Universal need
2. **Code Duplication** (2-3 weeks) - High value, clear ROI
3. **Vulnerability Scanning** (3-4 weeks) - Security is critical
4. **Secret Detection** (2-3 weeks) - Prevents disasters
5. **Test Coverage Integration** (3-4 weeks) - Already in your user stories
6. **DORA Metrics** (3-4 weeks) - Industry standard

**Total**: ~17-25 weeks (4-6 months) for critical features

### High Value (Build Second) üü°
7. **Technical Debt Tracking** (3-4 weeks)
8. **Code Smells Detection** (3-4 weeks)
9. **Performance Anti-Patterns** (4-5 weeks)
10. **Test Quality Metrics** (3-4 weeks)
11. **Documentation Coverage** (2-3 weeks)
12. **Knowledge Distribution** (2-3 weeks)
13. **PR Metrics** (2-3 weeks)
14. **Modularity Metrics** (3-4 weeks)

**Total**: ~24-34 weeks (6-8 months)

### Nice to Have (Build Third) üü¢
15. **Resource Usage Analysis** (3-4 weeks)
16. **Flow Metrics** (2-3 weeks)
17. **Code Churn** (2-3 weeks)
18. **Circular Dependencies** (2-3 weeks)
19. **Code Review Quality** (2-3 weeks)
20. **Pair Programming Metrics** (2-3 weeks)

**Total**: ~14-19 weeks (3-5 months)

---

## Recommended Roadmap

### Phase 1: MVP (Months 1-3)
- Complete current user stories (from CODEBASE_EVALUATION.md)
- Add critical metrics (complexity, duplication, secrets)
- Basic SaaS platform

### Phase 2: Differentiation (Months 4-6)
- Add vulnerability scanning
- Implement DORA metrics
- Technical debt tracking
- Performance anti-patterns

### Phase 3: Comprehensive Platform (Months 7-12)
- All high-value metrics
- Advanced visualizations
- Team collaboration features
- Enterprise features

---

## Conclusion

**The Opportunity**: By adding these metrics, you transform from a "workflow visualization tool" into a **"Complete Code Intelligence Platform"**.

**Most Valuable Additions**:
1. **Code Quality Suite**: Complexity + duplication + code smells (6-8 weeks)
2. **Security Suite**: Vulnerabilities + secrets + OWASP (7-9 weeks)
3. **Productivity Suite**: DORA + PR metrics + flow (7-9 weeks)
4. **Testing Suite**: Coverage + quality + recommendations (6-8 weeks)

**Estimated Total Effort**: ~55-85 weeks (14-21 months) for ALL metrics

**Realistic Plan**: Focus on critical + high-value = ~41-59 weeks (10-15 months) for comprehensive platform

---

**Next Steps**:
1. Validate these metrics with target customers
2. Prioritize based on customer feedback
3. Start with critical metrics (complexity, duplication, secrets)
4. Build in sprints, release iteratively
